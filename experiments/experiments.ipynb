{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.198955Z",
     "start_time": "2020-10-28T08:15:32.480547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "console.log('Starting front end url_querystring_target comm target');\n",
       "const comm = Jupyter.notebook.kernel.comm_manager.new_comm('url_querystring_target', {'init': 1});\n",
       "comm.send({'ipyparams_browser_url': window.location.href});\n",
       "console.log('Sent window.location.href on url_querystring_target comm target');\n",
       "\n",
       "comm.on_msg(function(msg) {\n",
       "    console.log(msg.content.data);\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 111111 ############\n",
      "############ 222222 ############\n",
      "############ 333333 ############\n",
      "xlime ALPHA: 0.05\n",
      "[{'base_url': '/', 'hostname': 'localhost', 'notebook_dir': '/cshome/motalleb', 'password': False, 'pid': 5979, 'port': 8077, 'secure': False, 'token': '842abddec5331e672bfe7fdefd1567b7271962ae8b84c294', 'url': 'http://localhost:8077/'}]\n",
      "backup_file\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 1\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "import pathos\n",
    "# from multiprocessing import Pool, TimeoutError\n",
    "from importlib import reload  \n",
    "import ipykernel\n",
    "import requests\n",
    "# Alternative that works for both Python 2 and 3:\n",
    "from requests.compat import urljoin\n",
    "from notebook.notebookapp import list_running_servers\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "# np.random.seed(seed=RANDOM_SEED)\n",
    "const_random_state = RandomState(RANDOM_SEED)\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel('CRITICAL')\n",
    "\n",
    "import experiments_config\n",
    "import ipyparams\n",
    "\n",
    "sys.path.append(experiments_config.SIGDIRECT_PATH)\n",
    "# from associative_classifier import AssociativeClassifier\n",
    "\n",
    "############ importing LIME   ############\n",
    "if experiments_config.LIME_PATH not in sys.path:\n",
    "    sys.path.append(experiments_config.LIME_PATH)\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "############ importing XLIME  ############\n",
    "if experiments_config.XLIME_PATH not in sys.path:\n",
    "    sys.path.append(experiments_config.XLIME_PATH)\n",
    "import xlime as xlime\n",
    "import xlime.lime_tabular\n",
    "############ importing SHAP   ############\n",
    "import shap\n",
    "############ importing Anchor ############\n",
    "import anchor\n",
    "import anchor.anchor_tabular\n",
    "\n",
    "############ importing RBO metric ############\n",
    "import rbo\n",
    "import dlukes_rbo\n",
    "\n",
    "def _reload_libs():\n",
    "    global xlime, lime\n",
    "    xlime = reload(xlime)\n",
    "    xlime.lime_tabular = reload(xlime.lime_tabular)\n",
    "    lime = reload(lime)\n",
    "    lime.lime_tabular = reload(lime.lime_tabular)\n",
    "\n",
    "def get_notebook_name():\n",
    "    kernel_id = re.search('kernel-(.*).json',\n",
    "                          ipykernel.connect.get_connection_file()).group(1)\n",
    "    servers = list_running_servers()\n",
    "    print(list(servers))\n",
    "    for ss in servers:\n",
    "        response = requests.get(urljoin(ss['url'], 'api/sessions'),\n",
    "                                params={'token': ss.get('token', '')})\n",
    "        for nn in json.loads(response.text):\n",
    "            if nn['kernel']['id'] == kernel_id:\n",
    "                return nn['notebook']['path'].split('/')[-1]\n",
    "    return \"backup_file\"\n",
    "print(get_notebook_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.231244Z",
     "start_time": "2020-10-28T08:15:38.201534Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_df(filename, has_index, header_index=None):\n",
    "    df = pd.read_csv(filename, sep=',', header=header_index, na_values='?')\n",
    "    if has_index:\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "    return df\n",
    "\n",
    "def get_test_df(filename, has_index, header_index=None):\n",
    "    df = pd.read_csv(filename, sep=',', header=header_index, na_values='?')\n",
    "    if has_index:\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(train_df, test_df, class_index, dataset_name):\n",
    "    # removing class label, so we can call get_dummies on the rest\n",
    "    train_df.rename(columns={list(train_df.columns)[class_index]:'class'}, inplace=True)\n",
    "    train_class = train_df['class']\n",
    "    train_df.drop(columns=['class'], inplace=True)\n",
    "    \n",
    "    test_df.rename(columns={list(test_df.columns)[class_index]:'class'}, inplace=True)\n",
    "    test_class = test_df['class']\n",
    "    test_df.drop(columns=['class'], inplace=True)\n",
    "\n",
    "    # process categorical data\n",
    "    infer_types = []\n",
    "    df = pd.concat([train_df, test_df])\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype=='object':\n",
    "            infer_types.append(\"{}_CAT\".format(column))\n",
    "        else:\n",
    "            infer_types.append(\"{}_NUM\".format(column))\n",
    "    datasets_info_dict[dataset_name]['_NUM'] = sum(['_NUM' in x for x in infer_types])\n",
    "    datasets_info_dict[dataset_name]['_CAT'] = sum(['_CAT' in x for x in infer_types])\n",
    "#     print('INFER_TYPES:', infer_types)\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    train_df = df[:train_df.shape[0]]\n",
    "    test_df  = df[train_df.shape[0]:]\n",
    "\n",
    "    assert set(train_df.columns)==set(test_df.columns)\n",
    "\n",
    "    # process numerical data (standardization is independent[?] for train/test splits)\n",
    "    continuous_column_names = [x for x in list(train_df.columns) if not '_' in str(x)]\n",
    "    for column in continuous_column_names:\n",
    "        # standardazing the column\n",
    "        scaler = StandardScaler()\n",
    "        train_df[column] = scaler.fit_transform(train_df[column].to_numpy().reshape((-1,1)))\n",
    "        test_df[column]  = scaler.transform(test_df[column].to_numpy().reshape((-1,1)))\n",
    "        \n",
    "        # set NaN to 0\n",
    "        train_df[column].fillna(0., inplace=True)\n",
    "        test_df[column].fillna(0., inplace=True)\n",
    "        \n",
    "    train_df['class'] = train_class\n",
    "    test_df['class']  = test_class\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "def get_data(dataset_name, dataset_info):\n",
    "    # input: name of the dataset, and a dictionary containing its info\n",
    "    # process input arguments\n",
    "    file_counts  = 2 if dataset_info['SEPARATE_FILES'] else 1\n",
    "    class_index  = dataset_info['CLASS_INDEX']\n",
    "    has_index    = dataset_info['HAS_INDEX']\n",
    "    header_index = dataset_info['HEADER_ROW_NUMBER']\n",
    "    \n",
    "    # set train/test filenames\n",
    "    if file_counts==2:\n",
    "        filenames = [os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['TRAIN_FILENAME']), \n",
    "                     os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['TEST_FILENAME'])]\n",
    "    else:\n",
    "        filenames = [os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['COMBINED_FILENAME'])]\n",
    "\n",
    "    if file_counts==1:\n",
    "        # load from file\n",
    "        df = get_train_df(filenames[0], has_index, header_index)\n",
    "        # shuffle\n",
    "        df = df.sample(frac=1, random_state=const_random_state)\n",
    "\n",
    "        train_size = int(df.shape[0] * TRAIN_RATIO)\n",
    "        train_df = df.iloc[:train_size]\n",
    "        test_df  = df.iloc[train_size:]\n",
    "        \n",
    "    else:\n",
    "        # load from file\n",
    "        train_df = get_train_df(filenames[0], has_index, header_index)\n",
    "        test_df  = get_test_df(filenames[1], has_index, header_index)\n",
    "        # shuffle\n",
    "        train_df = train_df.sample(frac=1, random_state=const_random_state)\n",
    "        test_df  = test_df.sample(frac=1, random_state=const_random_state)\n",
    "\n",
    "    # some of the preprocessing (one hot encoding + missing values + standardisation)\n",
    "    dataset_info['initial_features'] = train_df.shape[1] -1\n",
    "    \n",
    "    train_df, test_df = preprocess_data(train_df, test_df, class_index, dataset_name)\n",
    "    \n",
    "    dataset_info['features'] = train_df.shape[1] -1\n",
    "    dataset_info['initial_train_size'] = train_df.shape[0]\n",
    "    dataset_info['initial_test_size'] = test_df.shape[0]\n",
    "    dataset_info['original_train_df'] = train_df.copy()\n",
    "    dataset_info['original_test_df']  = test_df.copy()\n",
    "    \n",
    "    if shrink_train_size and train_df.shape[0]>desired_train_size:\n",
    "        train_df = train_df[:desired_train_size]\n",
    "    if shrink_test_size  and test_df.shape[0]>desired_test_size:\n",
    "        test_df = test_df[:desired_test_size]\n",
    "    dataset_info['train_size'] = train_df.shape[0]\n",
    "    dataset_info['test_size'] = test_df.shape[0]\n",
    "        \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.335331Z",
     "start_time": "2020-10-28T08:15:38.233736Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf(train_df, clf_type, info=None):\n",
    "    if clf_type.lower()=='dt':\n",
    "        return get_clf_dt(train_df, info['max_explanation_size'])\n",
    "    if clf_type.lower()=='lr':\n",
    "        return get_clf_lr(train_df, info['max_explanation_size'])\n",
    "    raise Exception('Wrong interpretable model, select from \"lr\", \"dt\"')\n",
    "\n",
    "def get_features(classifier_type, clf, row, info_dict):\n",
    "    if classifier_type=='sd':\n",
    "        clf_features = get_features_sd(clf, row)\n",
    "    elif classifier_type=='dt':\n",
    "        clf_features = get_features_dt(clf, row)\n",
    "    elif classifier_type=='lr':\n",
    "        clf_features = get_features_lr(clf, row, \n",
    "                                       info_dict['max_features'], \n",
    "                                       info_dict['num_labels'], \n",
    "                                       info_dict['predicted_label_index'])\n",
    "    else:\n",
    "        print('Incorrect classifier type. terminating ...', classifier_type)\n",
    "        raise Exception(\"Incorrect classifier type\")\n",
    "    return clf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.449839Z",
     "start_time": "2020-10-28T08:15:38.337561Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_dt(train_df, max_depth):\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(random_state=const_random_state, max_depth=max_depth)\n",
    "    clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "    return clf\n",
    "\n",
    "def test_classifier_dt(clf, test_df):\n",
    "    predictions = clf.predict(test_df.drop('class', axis=1))    \n",
    "    acc = sklearn.metrics.accuracy_score(test_df['class'], predictions)\n",
    "    return acc\n",
    "\n",
    "def get_features_dt(clf, row):\n",
    "    feature = clf.tree_.feature\n",
    "    leave_id = clf.apply(row.values.reshape(1, -1))\n",
    "    node_indicator = clf.decision_path([row])\n",
    "    features = OrderedDict() # using as an ordered set\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "#     node_index = node_indicator.indices[:] #node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "\n",
    "    for node_id in node_index:\n",
    "        if leave_id[0] == node_id:  # <-- changed != to ==\n",
    "            continue # <-- comment out\n",
    "        else: # < -- added else to iterate through decision nodes\n",
    "#             features.append(feature[node_id]+1)\n",
    "            if feature[node_id]+1 not in features:\n",
    "                features[feature[node_id]+1] = None\n",
    "#             else:\n",
    "#                 print('redundant!!!')\n",
    "    final_features = [*features]\n",
    "#     print(final_features)\n",
    "    return final_features\n",
    "\n",
    "def get_dt_avg_explanation(dt_clf, train_df):\n",
    "    # find the depth for each training instance, and then return the average among them\n",
    "#     decision_paths = dt_clf.decision_path(train_df).toarray()\n",
    "#     uniques = np.unique(decision_paths, axis=1)\n",
    "#     print(decision_paths[:5])\n",
    "#     print(np.count_nonzero(decision_paths, axis=1).max()-1, np.count_nonzero(decision_paths, axis=1).min()-1)\n",
    "#     return np.count_nonzero(decision_paths, axis=1).mean() - 1 # they all have an extra node which is the leaf\n",
    "    lens = 0\n",
    "    for _,row in train_df.iterrows():\n",
    "        exp = get_features_dt(dt_clf, row)\n",
    "        lens += len(exp)\n",
    "    return lens/train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.641078Z",
     "start_time": "2020-10-28T08:15:38.451740Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_lr(train_df, max_features):\n",
    "    try_cs1 = np.arange(1.,0,-.1)\n",
    "    try_cs2 = np.arange(.1,0,-.01)\n",
    "    try_cs3 = np.arange(.01,0,-.001)\n",
    "    \n",
    "    done = False\n",
    "    for c in try_cs1:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "    if done:\n",
    "        return temp_clf\n",
    "    for c in try_cs2:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "    if done:\n",
    "        return temp_clf\n",
    "    for c in try_cs3:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "#     print('c:', c)\n",
    "    return temp_clf\n",
    "    \n",
    "def test_classifier_lr(clf, test_df):\n",
    "    predictions = clf.predict(test_df.drop('class', axis=1))    \n",
    "    acc = sklearn.metrics.accuracy_score(test_df['class'], predictions)\n",
    "    return acc\n",
    "\n",
    "def get_features_lr(clf, row, max_features, num_classes, label_index):\n",
    "    if num_classes<=2:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = label_index\n",
    "    all_params = clf.coef_\n",
    "    return set(np.where(all_params[idx]!=0.)[0]+1)\n",
    "#     return set(np.argsort(all_params[idx])[:].tolist())\n",
    "\n",
    "def get_lr_avg_explanation(clf, train_df):\n",
    "    return max([len(x.nonzero()[0]) for x in clf.coef_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.861488Z",
     "start_time": "2020-10-28T08:15:38.642975Z"
    }
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "shrink_train_size = True\n",
    "desired_train_size = 100\n",
    "\n",
    "shrink_test_size = True\n",
    "desired_test_size = 100\n",
    "\n",
    "# used if data is not already split\n",
    "TRAIN_RATIO = 0.75\n",
    "\n",
    "PROCESS_COUNT = 50\n",
    "\n",
    "# compute averages over all instances, or over the ones that the labels agree\n",
    "fidelity_division = True\n",
    "\n",
    "# how many runs to make sure fidelity is respected in BARBE/XLIME\n",
    "REPEAT_COUNT = 1\n",
    "\n",
    "DATA_ROOT_DIR = experiments_config.DATA_ROOT_DIR\n",
    "datasets_info_dict = experiments_config.all_datasets_info_dict.copy()\n",
    "datasets_info_dict = {\n",
    "                    'glass':experiments_config.all_datasets_info_dict['glass'],\n",
    "#                      'wine':experiments_config.all_datasets_info_dict['wine'],\n",
    "#                      'hungarian':experiments_config.all_datasets_info_dict['hungarian'],\n",
    "#                      'hepatitis':experiments_config.all_datasets_info_dict['hepatitis'],\n",
    "#                      'poker':experiments_config.all_datasets_info_dict['poker'],\n",
    "                     }\n",
    "\n",
    "info = {'max_explanation_size':5}\n",
    "\n",
    "remove_datasets = [\n",
    "    'online_shoppers_intention', # slow with 1k!\n",
    "    'breast-cancer', # very slow even in 200!, but good results!\n",
    "    'car', # has a small tree! f1=55, fidel:82\n",
    "    'nursery', # precision=1, recall=.5\n",
    "    'adult', # slow, 1k won't finish in 2h\n",
    "]\n",
    "# remove_datasets = ['online_shoppers_intention']\n",
    "\n",
    "for x in remove_datasets:\n",
    "    if x in  datasets_info_dict:\n",
    "        del datasets_info_dict[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of these experiments:\n",
    "I want to know more about the datasets I use, and have the enough knowledge about them so I could use different type of datasets (in terms of size/feature/complexity) in my experiments.\n",
    "\n",
    "Given say onnly 10 features, we can have feature spaces that are very complex, or very simple. The same thing can happen when we have 100 features. So, I need datasets that cover all such cases.\n",
    "\n",
    "\n",
    "I plan to use train sets to a) train the classifiers, and b) tune BARBE. Since this is my own training, I can limit the size of the train set used in the experiments to a reasonable number like 200.\n",
    "#### Obtaining General info about datasets\n",
    "1. Number of features, train instances, test instances\n",
    "2. \n",
    "\n",
    "#### Understanding how different interpretable classifiers work w.r.t. these datasets\n",
    "1. Classifier accuracy on these datasets\n",
    "2. info unique to each classifier:\n",
    "    2.1. DT: average depth of the tree \n",
    "    2.2. Rule-based: average number of rules for each instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.055328Z",
     "start_time": "2020-10-28T08:15:38.862801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for dataset, dataset_info in datasets_info_dict.items():\n",
    "    train_df, test_df = get_data(dataset, dataset_info)\n",
    "    dataset_info['train_df'] = train_df\n",
    "    dataset_info['test_df'] = test_df\n",
    "    dataset_info['nclasses'] = train_df['class'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.226153Z",
     "start_time": "2020-10-28T08:15:39.057630Z"
    }
   },
   "outputs": [],
   "source": [
    "for dataset, dataset_info in datasets_info_dict.items():\n",
    "    ############ Decision Tree ############\n",
    "    if 'dt_clf' not in dataset_info:\n",
    "        dt_clf = get_clf(dataset_info['original_train_df'], 'dt', info)\n",
    "        avg_explanation_len = get_dt_avg_explanation(dt_clf, dataset_info['train_df'].drop(columns=['class']))\n",
    "        dataset_info['dt_len'] = avg_explanation_len\n",
    "        dataset_info['dt_acc'] = test_classifier_dt(dt_clf, dataset_info['test_df'])\n",
    "        dataset_info['dt_clf'] = dt_clf\n",
    "    ############ Logistic Regression ############\n",
    "    if 'lr_clf' not in dataset_info:\n",
    "        lr_clf = get_clf(dataset_info['original_train_df'], 'lr', info)\n",
    "        avg_explanation_len = get_lr_avg_explanation(lr_clf, dataset_info['train_df'].drop(columns=['class']))\n",
    "        dataset_info['lr_len'] = avg_explanation_len\n",
    "        dataset_info['lr_acc'] = test_classifier_lr(lr_clf, dataset_info['test_df'])\n",
    "        dataset_info['lr_clf'] = lr_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.297860Z",
     "start_time": "2020-10-28T08:15:39.227753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Datasets:', len(experiments_config.all_datasets_info_dict), '-->', len(datasets_info_dict))\n",
    "# for dataset, dataset_info in datasets_info_dict.items():\n",
    "#     print(dataset)\n",
    "#     pprint({x:y for (x,y) in dataset_info.items() if ('_df' not in x) and ('_clf' not in x)})\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.595220Z",
     "start_time": "2020-10-28T08:15:39.299264Z"
    }
   },
   "outputs": [],
   "source": [
    "# raise Exception(\"stop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuning BARBE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each instance and extract explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.759561Z",
     "start_time": "2020-10-28T08:15:39.596550Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nonzero_features_lime(row, clf, num_features, explainer, num_labels, \n",
    "                              max_valid_features, predicted_label_index, num_samples, xlime_mode, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    classes = list(range(num_labels))\n",
    "    if isinstance(explainer, xlime.lime_tabular.LimeTabularExplainer):\n",
    "        x = explainer.explain_instance(row.to_numpy(), \n",
    "                                   clf.predict_proba, \n",
    "                                   num_features=max_valid_features, \n",
    "                                   labels=[predicted_label_index],\n",
    "                                   num_samples=num_samples,\n",
    "                                   classes=classes,\n",
    "                                   xlime_mode=xlime_mode,\n",
    "                                   model_regressor='alaki' # SigDirect\n",
    "#                                    model_regressor=sklearn.tree.DecisionTreeRegressor(\n",
    "#                                                        criterion=\"mae\",\n",
    "#                                                        max_depth=max_valid_features, \n",
    "#                                        )\n",
    "#                                   model_regressor=sklearn.linear_model.LinearRegression()\n",
    "#                                   model_regressor=GaussianNB()\n",
    "                                  )\n",
    "#         print(x)\n",
    "        fidelity = x.fidelity\n",
    "    else:\n",
    "        x = explainer.explain_instance(row.to_numpy(), \n",
    "                                   clf.predict_proba, \n",
    "                                   num_features=max_valid_features, \n",
    "                                   labels=[predicted_label_index],\n",
    "                                   num_samples=num_samples,\n",
    "                                   \n",
    "                                  )\n",
    "#         print(x.local_pred)\n",
    "#         fidelity = 1.0\n",
    "        if x.local_pred>=1.0/num_labels:\n",
    "            fidelity = 1.0\n",
    "        else:\n",
    "            fidelity = 0.0\n",
    "    feature_score_pairs = x.as_map()[predicted_label_index]\n",
    "#     print('feature_score_pairs.shape', len(feature_score_pairs))\n",
    "    feature_score_pairs = [(x[0]+1, x[1]) for x in feature_score_pairs if x[1]!=0.]\n",
    "    feature_score_pairs = sorted(feature_score_pairs, key=lambda x:x[1], reverse=True)\n",
    "    nonzero_features = [x[0] for x in feature_score_pairs[:max_valid_features]]\n",
    "\n",
    "    if fidelity==0.0:##\n",
    "        nonzero_features = []\n",
    "    return nonzero_features, fidelity\n",
    "\n",
    "def get_nonzero_features_anchor(row, clf, num_features, explainer, \n",
    "                              max_valid_features, predicted_label_index, num_samples, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "#         def explain_instance(self, data_row, classifier_fn, threshold=0.95,\n",
    "#                           delta=0.1, tau=0.15, batch_size=100,\n",
    "#                           max_anchor_size=None,\n",
    "#                           desired_label=None,\n",
    "#                           beam_size=4, **kwargs):\n",
    "\n",
    "#     print('shape:', row.to_numpy().shape)\n",
    "    x = explainer.explain_instance(row.to_numpy().reshape(1,-1), \n",
    "                                   clf.predict, \n",
    "                                   max_anchor_size=max_valid_features, \n",
    "                                   desired_label=predicted_label_index,\n",
    "#                                    num_samples=num_samples,\n",
    "                                   coverage_samples=num_samples,\n",
    "                                  )\n",
    "#     print(x.names(), x.features())\n",
    "    features = x.features()#[predicted_label_index]\n",
    "    features = [x+1 for x in features]\n",
    "#     feature_score_pairs = [(x[0]+1, x[1]) for x in feature_score_pairs if x[1]!=0.]\n",
    "#     nonzero_features = [x[0] for x in feature_score_pairs]\n",
    "    return features\n",
    "\n",
    "def get_nonzero_features_shap(row, clf, num_features, explainer, \n",
    "                              max_valid_features, predicted_label_index, num_samples, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    x = explainer.shap_values(row, nsamples=num_samples)\n",
    "    features_scores = x[predicted_label_index]\n",
    "#     print(features_scores)\n",
    "    sorted_features = sorted(list(enumerate(features_scores)), key=lambda x:abs(x[1]), reverse=True)\n",
    "    ret = [x[0]+1 for x in sorted_features[:max_valid_features]]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.865783Z",
     "start_time": "2020-10-28T08:15:39.761157Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_instance(args):    \n",
    "    ((idx, row), clf, classifier_type, num_features, num_labels, \n",
    "                      explainer, num_samples, xlime_mode, seed, ordered_class_labels, method, max_features) = args\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "#     print(idx)\n",
    "    fidelity = 1\n",
    "    predicted_label_index = ordered_class_labels.index(clf.predict(row.values.reshape(1, -1))[0])\n",
    "    clf_features = get_features(classifier_type, clf, row, {'max_features':max_features, 'num_labels':num_labels, 'predicted_label_index':predicted_label_index})\n",
    "    if method==\"XLIME\":\n",
    "#         my_modes = [[\"FOURTEEN\", \"SIXTEEN\"], [\"SEVENTEEN\", \"EIGHTEEN\"], \"FIFTEEN\" ]\n",
    "#         my_modes = [[\"FOURTEEN\", \"SEVENTEEN\"]] \n",
    "        my_modes = [\"FOURTEEN\", \"SEVENTEEN\", \"SIXTEEN\", \"EIGHTEEN\"]\n",
    "        explainer_features = []\n",
    "        temp_seed = seed\n",
    "        for i in range(REPEAT_COUNT):\n",
    "            b, fidelity = get_nonzero_features_lime(row, clf, num_features, \n",
    "                                                  explainer, num_labels, max_features,                                                      \n",
    "                                                  predicted_label_index, num_samples, \n",
    "#                                                   [\"FOURTEEN\",\"FIFTEEN\"],\n",
    "                                                  my_modes[i] , # \"FOURTEEN\"\n",
    "                                                  seed, # temp_seed, # seed, # \n",
    "                                                   )\n",
    "            temp_seed += 11\n",
    "            if fidelity==1.0:\n",
    "#                 if i>0:\n",
    "#                     print(i)\n",
    "                if len(explainer_features)>0:\n",
    "                    explainer_features.extend([x for x in b if x not in set(explainer_features)])\n",
    "                else:\n",
    "                    explainer_features = b\n",
    "                break\n",
    "        \n",
    "        explainer_features = explainer_features[:max_features]\n",
    "    elif method==\"LIME\":\n",
    "        explainer_features, fidelity = get_nonzero_features_lime(row, clf, num_features, \n",
    "                                                  explainer, num_labels, max_features,                                                      \n",
    "                                                  predicted_label_index, num_samples, xlime_mode,\n",
    "                                                  seed)\n",
    "\n",
    "    elif method==\"ANCHOR\":\n",
    "        explainer_features = set(get_nonzero_features_anchor(row, clf, num_features, \n",
    "                                                  explainer, max_features, \n",
    "                                                  predicted_label_index, num_samples,\n",
    "                                                  seed))\n",
    "    elif method==\"SHAP\":\n",
    "        explainer_features = set(get_nonzero_features_shap(row, clf, num_features, \n",
    "                                                  explainer, max_features, \n",
    "                                                  predicted_label_index, num_samples,\n",
    "                                                  seed))\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method selected\", method)\n",
    "#     print(idx, \n",
    "#           fidelity,\n",
    "#           list(clf_features), \n",
    "#           list(explainer_features),\n",
    "#           sum([x in clf_features for x in explainer_features])/len(explainer_features) if len(explainer_features)>0 else 0.0,\n",
    "#           sum([x in explainer_features for x in clf_features])/len(clf_features) if len(clf_features)>0 else 0.0)\n",
    "    fout.write(\"{}, {}, {}, {}\\n\".format(idx, \n",
    "                                         list(clf_features), \n",
    "                                         list(explainer_features), \n",
    "                                         sum([x in clf_features for x in explainer_features])/len(explainer_features) if len(explainer_features)>0 else 0.0,\n",
    "                                         sum([x in explainer_features for x in clf_features])/len(clf_features) if len(clf_features)>0 else 0.0))\n",
    "    return list(clf_features), list(explainer_features), fidelity\n",
    "    \n",
    "\n",
    "def evaluate_explanations_parallel(dataset_name, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, max_features, method='LIME', xlime_mode='ONE'):\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    train_df2 = train_df.drop('class', axis=1)\n",
    "    test_df2 = test_df.drop('class', axis=1)\n",
    "    ordered_class_labels = sorted(list(set(train_df['class'].values)))\n",
    "    columns = list(train_df2.columns)\n",
    "    categorical_features        = [x for x in columns if '_' in str(x)]\n",
    "    categorical_feature_indices = [columns.index(x) for x in columns if '_' in str(x)]\n",
    "    categorical_features_map    = {columns.index(x):x for x in columns if '_' in str(x)}\n",
    "    \n",
    "    all_features = train_df2.columns.values\n",
    "    print('dataset:', dataset_name, 'method:', method, 'seed:', seed, 'num_samples:', num_samples, 'test size:', test_df2.shape)\n",
    "    fout.write('dataset: {} method: {} seed: {} num_sampes: {} test size: {}\\n'.format(dataset_name, method, seed, num_samples, test_df2.shape))\n",
    "    if method=='XLIME':\n",
    "#         discretizers = ['decile', 'eightile', 'sixile', 'quartile'] \n",
    "#         for i in range(4):\n",
    "#             try:\n",
    "        explainer = xlime.lime_tabular.LimeTabularExplainer(train_df2.values, \n",
    "                                                           categorical_features=categorical_feature_indices, \n",
    "                                                           feature_names=all_features,\n",
    "                                                           verbose=False, \n",
    "                                                           class_names=ordered_class_labels,\n",
    "                                                           mode='classification',\n",
    "                                                           random_state=RandomState(seed), \n",
    "                                                           discretizer='decile',\n",
    "#                                                            discretizer=discretizers[i],\n",
    "    #                                                        training_labels=train_df['class'].values,\n",
    "                                                           feature_selection='none'\n",
    "                                                          )\n",
    "#                 print(discretizers[i])\n",
    "#                 break\n",
    "#             except Exception as e:\n",
    "#                 print(str(e))\n",
    "#                 pass\n",
    "    elif method=='LIME':\n",
    "        discretizers = ['decile', 'eightile', 'sixile', 'quartile'] \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                explainer = lime.lime_tabular.LimeTabularExplainer(train_df2.values,  \n",
    "                                                       categorical_features=categorical_feature_indices, \n",
    "                                                       feature_names=all_features,\n",
    "                                                       verbose=False, \n",
    "                                                       class_names=ordered_class_labels,\n",
    "                                                       mode='classification', \n",
    "                                                       sample_around_instance=around_instance, \n",
    "                                                       random_state=RandomState(seed),\n",
    "                                                       discretizer=discretizers[i],\n",
    "#                                                        training_labels=train_df['class'].values,\n",
    "                                                      )\n",
    "#                 print(discretizers[i])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                pass\n",
    "    elif method=='ANCHOR':\n",
    "#          def __init__(self, class_names, feature_names, data=None,\n",
    "#                  categorical_names=None, ordinal_features=[]):\n",
    "#         explainer = anchor_tabular.AnchorTabularExplainer(dataset.class_names, dataset.feature_names, \n",
    "#                                                           dataset.data, dataset.categorical_names)\n",
    "        explainer = anchor.anchor_tabular.AnchorTabularExplainer(ordered_class_labels,\n",
    "                                                       train_data=train_df2.values,  \n",
    "                                                       categorical_names=categorical_features_map, \n",
    "                                                       feature_names=all_features,\n",
    "#                                                        verbose=False, \n",
    "#                                                        class_names=ordered_class_labels, # error\n",
    "#                                                        mode='classification',  # error\n",
    "#                                                        sample_around_instance=around_instance,  # error\n",
    "#                                                        random_state=RandomState(seed), # error\n",
    "                                                       discretizer='decile'\n",
    "                                                      )\n",
    "#         explainer.fit(dataset.train, dataset.labels_train, dataset.validation, dataset.labels_validation)\n",
    "#         split_idx = int(train_df.shape[0] * 0.75)\n",
    "#         temp_train_df = train_df[:split_idx]\n",
    "#         temp_valid_df = train_df[split_idx:]\n",
    "#         print(temp_valid_df['class'].values)\n",
    "#         explainer.fit(train_df.drop('class', axis=1).values, train_df['class'].values,\n",
    "#                       train_df.drop('class', axis=1).values, train_df['class'].values,\n",
    "#                       discretizer='decile')\n",
    "\n",
    "    elif method=='SHAP':\n",
    "        explainer = shap.KernelExplainer(clf.predict_proba, train_df2.values) #, link=<shap.common.IdentityLink object>, **kwargs)\n",
    "    else:\n",
    "        raise Exception(\"incorrect explanation method provided to evaluate_explanations:\", method)\n",
    "        \n",
    "    num_labels = len(ordered_class_labels) # 0#max(train_df['class']) - train_df.shape[1]\n",
    "    \n",
    "    # modify this based on the type of the experiments (tuning --> train_df2)/(evaluating --> test_df2)\n",
    "#     chosen_dataset = train_df2\n",
    "    chosen_dataset = test_df2\n",
    "    \n",
    "    if method in ('XLIME', 'ANCHOR'):\n",
    "        with pathos.multiprocessing.ProcessPool(ncpus=PROCESS_COUNT) as pool:\n",
    "            ret = pool.map(evaluate_instance, zip(\n",
    "                                                  chosen_dataset.iterrows(),\n",
    "                                                  itertools.cycle([clf]),\n",
    "                                                  itertools.cycle([classifier_type]),\n",
    "                                                  itertools.cycle([chosen_dataset.shape[1]]),\n",
    "                                                  itertools.cycle([num_labels]),\n",
    "                                                  itertools.cycle([explainer]),\n",
    "                                                  itertools.cycle([num_samples]),\n",
    "                                                  itertools.cycle([xlime_mode]),\n",
    "                                                  itertools.cycle([seed]),\n",
    "                                                  itertools.cycle([ordered_class_labels]),\n",
    "                                                  itertools.cycle([method]),\n",
    "                                                  itertools.cycle([max_features])\n",
    "                                                 ),                                                                          \n",
    "                                             )\n",
    "    else:\n",
    "        ret = map(evaluate_instance, zip(\n",
    "                                                  chosen_dataset.iterrows(),\n",
    "                                                  itertools.cycle([clf]),\n",
    "                                                  itertools.cycle([classifier_type]),\n",
    "                                                  itertools.cycle([chosen_dataset.shape[1]]),\n",
    "                                                  itertools.cycle([num_labels]),\n",
    "                                                  itertools.cycle([explainer]),\n",
    "                                                  itertools.cycle([num_samples]),\n",
    "                                                  itertools.cycle([xlime_mode]),\n",
    "                                                  itertools.cycle([seed]),\n",
    "                                                  itertools.cycle([ordered_class_labels]),\n",
    "                                                  itertools.cycle([method]),\n",
    "                                                  itertools.cycle([max_features])\n",
    "                                                 ),                                                                          \n",
    "                                             )\n",
    "    all_clf_features, all_exp_features, fidelities = zip(*ret)\n",
    "    return all_clf_features, all_exp_features, fidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:40.102939Z",
     "start_time": "2020-10-28T08:15:39.867062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def analyze_outputs(all_features, initial_all_clf_features, initial_all_exp_features, fidelities):\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    if fidelity_division:\n",
    "        all_clf_features = [x for x,fidel in zip(initial_all_clf_features, fidelities) if fidel]\n",
    "        all_exp_features = [x for x,fidel in zip(initial_all_exp_features, fidelities) if fidel]\n",
    "    else:\n",
    "        all_clf_features = initial_all_clf_features\n",
    "        all_exp_features = initial_all_exp_features\n",
    "        \n",
    "    print(len(all_clf_features), sum(fidelities))\n",
    "#     print('shapes:', len(all_clf_features), len(all_exp_features))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([[i+1] for i,x in enumerate(all_features)])\n",
    "#     print('unique 1:', set(itertools.chain(*all_clf_features)))\n",
    "#     print('unique 2:', set(itertools.chain(*all_exp_features)))\n",
    "    print(classification_report(mlb.transform(all_clf_features), mlb.transform(all_exp_features), output_dict=True)['samples avg'])\n",
    "    fout.write(str(classification_report(mlb.transform(all_clf_features), mlb.transform(all_exp_features), output_dict=True)['samples avg'])+'\\n')\n",
    "\n",
    "    print('f0.5-score:', precision_recall_fscore_support(mlb.transform(all_clf_features), mlb.transform(all_exp_features), beta=0.5, average='samples')[2])\n",
    "    fout.write('f0.5-score: ' + str(precision_recall_fscore_support(mlb.transform(all_clf_features), mlb.transform(all_exp_features), beta=0.5, average='samples')[2])+'\\n')\n",
    "\n",
    "    print('Jaccard similarity:', jaccard_score(mlb.transform(all_clf_features), mlb.transform(all_exp_features), \n",
    "                                               average='samples'))\n",
    "    fout.write('Jaccard similarity: ' + str(jaccard_score(mlb.transform(all_clf_features), mlb.transform(all_exp_features), \n",
    "                                               average='samples'))+'\\n')\n",
    "    # TODO 1: add RBO to this method\n",
    "    # TODO 2: Are SHAP, Anchor outputs ordered?\n",
    "    rbo_scores = []\n",
    "    rbo_scores2 = []\n",
    "    for clf_features, exp_features in zip(all_clf_features, all_exp_features):\n",
    "#         rbo_score = rbo.RankingSimilarity(clf_features, exp_features).rbo()\n",
    "#         rbo_scores.append(rbo_score)\n",
    "#         print('RBO: {} {} {}'.format(rbo_score, clf_features, exp_features))\n",
    "        if len(clf_features)>0 and len(exp_features)>0:\n",
    "            rbo_score2 = dlukes_rbo.rbo(clf_features, exp_features, p=0.5)\n",
    "#             rbo_scores2.append(rbo_score2.min + rbo_score2.res/2)\n",
    "            rbo_scores2.append(rbo_score2.ext)\n",
    "        else:\n",
    "            rbo_score2 = 0.\n",
    "            rbo_scores2.append(0.)\n",
    "#         print('RBO: {}'.format(rbo_score2))\n",
    "#     print('RBO AVG: {}'.format(sum(rbo_scores)/len(rbo_scores)))\n",
    "    print('RBO2 AVG: {}'.format(sum(rbo_scores2)/len(rbo_scores2)))\n",
    "    fout.write('RBO2 AVG: {}\\n'.format(sum(rbo_scores2)/len(rbo_scores2)))\n",
    "    print('FIDELITY AVG: {}'.format(sum(fidelities)/len(fidelities)))\n",
    "    fout.write('FIDELITY AVG: {}\\n'.format(sum(fidelities)/len(fidelities)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:18:02.252632Z",
     "start_time": "2020-10-28T08:15:40.104356Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "glass\n",
      "dataset: glass method: LIME seed: 1 num_samples: 1000 test size: (54, 9)\n",
      "52 52.0\n",
      "{'precision': 0.6730769230769231, 'recall': 0.7298076923076924, 'f1-score': 0.6927960927960929, 'support': 238}\n",
      "f0.5-score: 0.6799096736596737\n",
      "Jaccard similarity: 0.5576923076923076\n",
      "RBO2 AVG: 0.7572716346153844\n",
      "FIDELITY AVG: 0.9629629629629629\n",
      "dataset: glass method: LIME seed: 1 num_samples: 2000 test size: (54, 9)\n",
      "52 52.0\n",
      "{'precision': 0.7038461538461538, 'recall': 0.7721153846153845, 'f1-score': 0.7268620268620267, 'support': 238}\n",
      "f0.5-score: 0.7117278554778554\n",
      "Jaccard similarity: 0.6001373626373627\n",
      "RBO2 AVG: 0.7631610576923075\n",
      "FIDELITY AVG: 0.9629629629629629\n",
      "dataset: glass method: LIME seed: 1 num_samples: 3000 test size: (54, 9)\n",
      "51 51.0\n",
      "{'precision': 0.7137254901960786, 'recall': 0.7715686274509803, 'f1-score': 0.7328976034858389, 'support': 234}\n",
      "f0.5-score: 0.7202465834818776\n",
      "Jaccard similarity: 0.6085901027077497\n",
      "RBO2 AVG: 0.7735294117647057\n",
      "FIDELITY AVG: 0.9444444444444444\n",
      "dataset: glass method: LIME seed: 1 num_samples: 4000 test size: (54, 9)\n",
      "51 51.0\n",
      "{'precision': 0.7019607843137254, 'recall': 0.765686274509804, 'f1-score': 0.7237472766884532, 'support': 233}\n",
      "f0.5-score: 0.7094622697563874\n",
      "Jaccard similarity: 0.5942110177404294\n",
      "RBO2 AVG: 0.7595179738562089\n",
      "FIDELITY AVG: 0.9444444444444444\n",
      "dataset: glass method: LIME seed: 1 num_samples: 5000 test size: (54, 9)\n",
      "50 50.0\n",
      "{'precision': 0.7199999999999999, 'recall': 0.7919999999999999, 'f1-score': 0.7434285714285714, 'support': 229}\n",
      "f0.5-score: 0.727909090909091\n",
      "Jaccard similarity: 0.6199047619047618\n",
      "RBO2 AVG: 0.7830416666666663\n",
      "FIDELITY AVG: 0.9259259259259259\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "glass\n",
      "dataset: glass method: LIME seed: 2 num_samples: 1000 test size: (54, 9)\n",
      "52 52.0\n",
      "{'precision': 0.6692307692307692, 'recall': 0.7355769230769231, 'f1-score': 0.6913919413919414, 'support': 238}\n",
      "f0.5-score: 0.6767919580419579\n",
      "Jaccard similarity: 0.5510531135531135\n",
      "RBO2 AVG: 0.7635616987179484\n",
      "FIDELITY AVG: 0.9629629629629629\n",
      "dataset: glass method: LIME seed: 2 num_samples: 2000 test size: (54, 9)\n",
      "52 52.0\n",
      "{'precision': 0.7076923076923077, 'recall': 0.7769230769230769, 'f1-score': 0.7311355311355313, 'support': 238}\n",
      "f0.5-score: 0.7157342657342658\n",
      "Jaccard similarity: 0.6024267399267399\n",
      "RBO2 AVG: 0.7613581730769229\n",
      "FIDELITY AVG: 0.9629629629629629\n",
      "dataset: glass method: LIME seed: 2 num_samples: 3000 test size: (54, 9)\n",
      "52 52.0\n",
      "{'precision': 0.7115384615384616, 'recall': 0.7740384615384616, 'f1-score': 0.732905982905983, 'support': 238}\n",
      "f0.5-score: 0.7188956876456877\n",
      "Jaccard similarity: 0.6065934065934064\n",
      "RBO2 AVG: 0.7643429487179485\n",
      "FIDELITY AVG: 0.9629629629629629\n",
      "dataset: glass method: LIME seed: 2 num_samples: 4000 test size: (54, 9)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9400b32ef485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#             # evaluate LIME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mouputs_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_explanations_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maround_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_explanation_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0manalyze_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouputs_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e586e3ff2c54>\u001b[0m in \u001b[0;36mevaluate_explanations_parallel\u001b[0;34m(dataset_name, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, max_features, method, xlime_mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m                                                  ),                                                                          \n\u001b[1;32m    190\u001b[0m                                              )\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mall_clf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_exp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_clf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_exp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e586e3ff2c54>\u001b[0m in \u001b[0;36mevaluate_instance\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                   \u001b[0mexplainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                   \u001b[0mpredicted_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlime_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                   seed)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"ANCHOR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6b66276c9d63>\u001b[0m in \u001b[0;36mget_nonzero_features_lime\u001b[0;34m(row, clf, num_features, explainer, num_labels, max_valid_features, predicted_label_index, num_samples, xlime_mode, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m                                    \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_valid_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_label_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                    \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                   )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m# Preventative code: if sparse, convert to csr format if not in csr format already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mdata_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Note in sparse case we don't subtract mean since data would become dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36m__data_inverse\u001b[0;34m(self, data_row, num_samples)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mundiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36mundiscretize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 ret[:, feature] = self.get_undiscretize_values(\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 )\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lime/discretize.py\u001b[0m in \u001b[0;36mget_undiscretize_values\u001b[0;34m(self, feature, values)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_max_unequal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_max_unequal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# by _rvs().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m_rvs\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;31m## Use basic inverse cdf algorithm for RV generation as default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m_ppf\u001b[0;34m(self, q, a, b)\u001b[0m\n\u001b[1;32m   7161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_truncnorm_ppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_munp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36mvf_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   6931\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvf_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6933\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mvf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6934\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvf_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6935\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectorize_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2165\u001b[0m                       for a in args]\n\u001b[1;32m   2166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m_truncnorm_ppf\u001b[0;34m(q, a, b)\u001b[0m\n\u001b[1;32m   7086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_norm_isf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7087\u001b[0m         \u001b[0mna\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7088\u001b[0;31m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7089\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_norm_ppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mna\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m_norm_cdf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theNotebook = 'final_anchor' # get_notebook_name().split('.')[0]\n",
    "fout = open('{}.log'.format(theNotebook), 'a', 1)\n",
    "samples_range = range(1000, 6000, 1000)\n",
    "# samples_range = range(100, 1100, 100)\n",
    "NUM_ITERATIONS = 5\n",
    "classifier_type = 'dt'\n",
    "around_instance = True\n",
    "_reload_libs()\n",
    "xlime_mode = [\"FOURTEEN\"]\n",
    "iteration_seed = 0\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    iteration_seed += 1\n",
    "    for dataset, dataset_info in datasets_info_dict.items():\n",
    "        print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print(dataset)\n",
    "        seed = iteration_seed\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            clf = dataset_info['{}_clf'.format(classifier_type)]\n",
    "            train_df = dataset_info['train_df']\n",
    "            test_df  = dataset_info['test_df']\n",
    "            all_features = train_df.drop('class', axis=1).columns.values\n",
    "            \n",
    "# #             # evaluate XLIME\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'XLIME', xlime_mode)\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "#             # evaluate LIME\n",
    "            for num_samples in samples_range:\n",
    "                ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'LIME')\n",
    "                analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "# #             # evaluate SHAP\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'SHAP')\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp)\n",
    "            \n",
    "# #             # evaluate Anchors\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'ANCHOR')\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('EXCEPTION:', str(e))\n",
    "            fout.write('EXCEPTION' + str(e)+'\\n')\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fourteen and then fifteen: 0.5, and then 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:18:02.254330Z",
     "start_time": "2020-10-28T08:15:32.565Z"
    }
   },
   "outputs": [],
   "source": [
    "### 0.1 * softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
